# [TM10027100] - High Performance Computing
This course reviews high-performance computing systems from both aspects of hardware and software. The course talks about the importance of parallel processing, parallel system architectures, parallel algorithm design, parallel programming, and performance evaluation methodologies.
Class materials are uploaded to Google Classroom, and students will do programming practice with their own PCs.
**Class code：5ht53s3**

## Class ([hpc_01](hpc_01/)) #1
### Notes about the first class: [README.md](hpc_01/README.md)
#### Resume:
- Importance of parallel processing
- Parallel system architectures
- Parallel algorithm design
- Parallel programming
- Performance evaluation methodologies

Class materials are available on Google Classroom, and students are expected to practice programming on their own PCs. The class code for Google Classroom is **5ht53s3**.

### Class Schedule Highlights:
1. **Introduction to HPC**: Overview of HPC systems and the significance of parallel computing.
2. **Parallel Architectures**: Discussion on modern parallel computing systems and power efficiency.
3. **Parallel Algorithm Design**: Techniques for designing algorithms that can be executed in parallel.
4. **MPI and OpenMP Programming**: Practical sessions on writing parallel programs using MPI and OpenMP.

### Key Concepts:
- **FLOPS**: Measurement of supercomputer performance.
- **Amdahl’s Law**: Principle describing the potential speedup from parallelizing a task.
- **CPU-GPU Heterogeneous Systems**: Systems combining CPUs and GPUs for optimized performance.

### Benefits of Parallel Computing:
- Faster runtime
- Ability to solve larger problems
- Improved energy efficiency

### Tools and Standards:
- **OpenMP**: For thread-level parallelism.
- **MPI**: For process-level parallelism.


## Class ([hpc_02](hpc_02/)) #2
### Notes about the second class: [README.md](hpc_02/README.md)
#### Resume:
- **Shared Memory Systems**: Understanding UMA and NUMA architectures.
- **Cache Coherence**: Mechanisms to maintain consistency in shared data.
- **Distributed Memory Systems**: Concepts of message passing and MPI.
- **Hybrid Systems**: Combining shared and distributed memory approaches.
- **Performance Metrics**: Evaluating latency, bandwidth, and network topology.
- **Vectorization**: Techniques for optimizing code on vector processors.
- **Compiler Optimization**: Using compiler directives for parallelization.
- **SIMD**: Single Instruction, Multiple Data processing.
- **Network Topologies**: Different network structures and their impact on performance.
- **Message Collision**: Handling communication conflicts in shared networks.